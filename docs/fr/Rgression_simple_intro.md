# La R√©gression lin√©aire simple 


1. Introduction 
	2. Rappel sur le machine learning supervis√© ?
	3. Le principe de r√©gression : approche intuitive  
2. La r√©gression lin√©aire simple en th√©orie
	3. Variables cible et variables explicative
	4. Explication des coefficient, constante et r√©sidus 
	5. Les hypoth√®ses derri√®re une r√©gression lin√©aire : lin√©arit√©, homoscedasticit√© et normalit√© des variables
	7. Estimation par la m√©thode des moindres carr√©s
	8. Analyse et validation du mod√®le : R2 et MSE  

8. La r√©gression lin√©aire simple en pratique avec la librairie scikit-learn


______________________

# Partie 1. Introduction  

Le machine learning une science multidisciplinaire qui a pour ambition de permettre √† un ordinateur de r√©soudre des probl√®mes complexes qui ne peuvent √™tre appr√©hend√©s par des algorithmes simples. On appelle algorithme simple une suite d'instruction ou conditions afin de calculer un r√©sultat peu complexe.

Selon Wikipedia, en math√©matiques, les r√©gressions recouvrent plusieurs m√©thodes d‚Äôanalyse statistique permettant d‚Äôapprocher une variable √† partir d‚Äôautres qui lui sont corr√©l√©es.

Dans ce cours on s'int√©ressera aux diff√©rentes techniques de r√©gressions et plus particuli√®rement √† la r√©gression lin√©aire simple. 
 
L'objectif de la r√©gression lin√©aire simple est de mod√©liser la relation entre une caract√©ristique unique/une variable explicative `x` et une r√©ponse/une variable cible `y`. 
 

## 1.1. Rappel sur le machine learning supervis√© ?

Le machine learning supervis√© est une branche du machine learning qui vise √† r√©soudre des probl√®mes pour lesquels on dispose d‚Äôexemples d√©j√† r√©solus. 

Par exemple on rassemble des donn√©es sur un √©chantillons de logements √† Paris qui d√©crivent leur localisation, divers caract√©ristiques ainsi que le montant du loyer. 

Si notre probl√®me est d‚Äôestimer le montant du loyer d‚Äôun logement qui n‚Äôest pas dans notre base, on construira √† partir de nos donn√©es un mod√®le qui estime le montant du loyer en fonction des caract√©ristiques du logement et on appliquera ce mod√®le (ou estimateur) au logement inconnu pour en estimer le loyer. Ce probl√®me rel√®ve de l‚Äôapprentissage supervis√© car au moment de construire notre mod√®le on connaissait les valeurs prises par la variable que l‚Äôon souhaite estimer, qu‚Äôon appelle la variable cible.


### 1.1.1. Pourquoi faire du machine learning supervis√© ? 

Une fois le probl√®me bien pos√©, c'est √† dire que la variable cible √† √©t√© choisi et que nous avons rassembl√© un certain nombre de variables explicatives, on peut r√©sumer les objectifs de l'apprentissage supervis√©e en trois grandes cat√©gories :


* **D√©crire** : On peut chercher √† comprendre les relations qui peuvent exister entre la variable cible ```Y``` et le(s) variable(s) explicative(s) `x` dans le cas d'une r√©gression simple et `X1, ..., Xp` dans le cas multiple afin par exemple de s√©lectionner celle qui sont le plus pertinentes, o√π obtenir une visualisation des comportements dans la population observ√©e (attention ici population est employ√© comme un terme statistique et peut aussi bien d√©signer des personnes, des pays, des transactions financi√®res etc‚Ä¶)
*   **Expliquer** : Lorsqu‚Äôon a une connaissance √† priori du sujet trait√©, comme c‚Äôest souvent le cas en √©conomie ou en biologie par exemple, l‚Äôobjectif est de construire un test qui permet de v√©rifier ou confirmer des r√©sultats th√©orique dans des situations pratiques.
*   **Pr√©dire** : Ici on met l‚Äôaccent sur la qualit√© d'estimation, on cherche √† construire un mod√®le qui permette de produire des pr√©dictions fiables pour de futures observations.



## 1.2. Le principe de r√©gression : approche intuitive  

On rappel que en statistique un mod√®le de r√©gression lin√©aire est un mod√®le qui cherche √† √©tablir une relation lin√©aire entre une variable `x` et une variable `y`. 

**L'objectif √©tant de pouvoir ensuite avec ce mod√®le faire des pr√©visions/pr√©dictions sur la variable `y` √† partir d'une variable `x` inconnue du mod√®le**

On peut prendre les exemples suivants : 

- Existe t'il une relation (lin√©aire) entre le nombre d'ann√©e d'exp√©rience comme d√©veloppeur not√© `x` et un certain salaire not√© `y`. 
- Existe t'il une relation (lin√©aire) entre la surface d'un appartement not√© `x` et un certain prix de vente not√© `y`. 

D√©veloppons notre premi√®re exemple avec les donn√©es abstaites ci-dessous :  


| exp√©rience | 0  | 3  | 6  | 8  |
|------------|----|----|----|----|
| salaire    | 35 | 45 | 65 | 80 |


Commen√ßons par tracer un nuage de point avec nos donn√©es. 

![Nuage de points](./intuitive_scatter.png) 


Apr√®s avoir repr√©sent√© ces points on va essayer de voir si il existe finalement une relation lin√©aire entre les ann√©es d'exp√©rience en tant que d√©veloppeur et le salaire propos√©. On peut reformuler cet √©nonc√© comme "est ce que plus j'ai d'ann√©es d'exp√©rience plus le salaire propos√© augmente". Pour r√©pondre √† cette questions nous allons suivre les √©tapes suivantes : 

1. Tracer une droite qui passe "au mieux" par ces points
2. Formaliser un crit√®re de s√©lection de "meilleure" droite
3. Analyser le r√©sultat de la droite 


Commen√ßons par tracer intuitivement la droite de r√©gression (en rouge sur le sch√©ma) qui passe "au mieux" par ces points. Ce qu'on entend par "au mieux" c'est un crit√®re de r√©ussite : la somme des √©carts mesur√©s (en vert sur le sch√©ma) entre tous les points et la droite doit √™tre le plus petit possible.  

![Nuage de points avec une droite](./intuitive_scatter_reg.pdf) 

Par ailleurs, on appelle cette droite, une courbe afine et nous avons en math√©matique une mani√®re sp√©cifique de la noter, nous y reviendrons dans la suite. 

Maintenant pour analyser cette droite il nous faut comme dit ci-dessus un certain crit√®re qui va nous servir √† mesurer la qualit√© de cette droite de r√©gression : que la somme des √©carts mesur√©s soit la plus petite possible on va donc pour cela aussi formaliser une mesure math√©matique que nous verrons dans la suite du chapitre. 


__________________________________________________


# Partie 2. La r√©gression lin√©aire simple en th√©orie


## 2.1. Variable cible et variable explicative

Selon Wikip√©dia, dans les math√©matiques sup√©rieures et en logique, une variable est un symbole repr√©sentant, a priori, un objet ind√©termin√©. On peut cependant ajouter des conditions sur cet objet, tel que l'ensemble ou la collection le contenant. On peut alors utiliser une variable pour marquer un r√¥le dans un pr√©dicat, une formule ou un algorithme, ou bien r√©soudre des √©quations et d'autres probl√®mes. 

Ce qui va nous int√©resser dans le cadre du machine learning supervis√©, comme √©nonc√© ci-dessus c'est de trouver un algorithme, une fonction, un modele (dans ce contexte on peut consid√©rer ces termes comme des synonymes) qui peut faire des pr√©visions/pr√©dictions d'une variable cible not√© `y` √† partir d'une variable `x` inconnue. 

On va donc distinguer deux type de variables : 

- **la variable cible ou target** souvent not√©e `Y` est la variable dont on souhaite estimer la valeur. Par exemple, si nous essayons de pr√©dire le salaire de quelqu‚Äôun en fonction de son nombre d‚Äôann√©es d‚Äôexp√©rience. La variable target `Y` correspond au salaire.
- **la variable explicative** souvent repr√©sent√©e par `X` est la variable que nous avons √† disposition ou que l'on a choisi qui va nous permettre de d√©terminer la valeur d'une variables cible `Y`. Par exemple, si nous essayons de pr√©dire le salaire de quelqu‚Äôun en fonction de son nombre d‚Äôann√©es d‚Äôexp√©rience. La variable explicative `X` correspond au nombre d‚Äôann√©es d‚Äôexp√©rience.


### 2.1.1 Les types de variables 

Avant de passer √† la suite il est important de comprendre les diff√©rents types de variables que vous pouvez rencontrer dans le cadre de nos diff√©rents cours. 

#### 2.1.1.1 Les variables quantitatives 

Selon Wikipedia, en statistique, une variable quantitative ou crit√®re qualitatif est une variable Ce lien renvoie vers une page d'homonymie ou grandeur qui peut √™tre repr√©sent√©e par des nombres sur lesquels les op√©rations arithm√©tiques de base ont un sens. 

On distingue deux types de variables quantitatives : 

- **Les variables discr√®tes** : elles prennent des valeurs enti√®res (exemple 1, 2, 3, 7). Elle ne peuvent pas etre des nombres √† virgule, on peut prendre l'exemple du nombre de fr√®res et soeurs, il est impossible d'avoir 1,5 soeur. 
- **Les variables continues** : elles prennent n'importe quelle valeur num√©rique, nombre entier ou bien √† virgule. On peut prendre l'exemple d'un relev√© de taille d'une population. 

#### 2.1.1.2 Les variables qualitatives

Selon Wikipedia, en statistique, une variable qualitative, une variable cat√©gorielle, ou bien un facteur est une variable qui prend pour valeur des modalit√©s, des cat√©gories ou bien des niveaux, par opposition aux variables quantitatives qui mesurent sur chaque individu une quantit√©.

On distingue trois types de variables qualitatives :

- **Les variables binaires** : elles ne prennent uniquement que deux modalit√©s comme les variable bool√©enne. Suivant les situations il est possible des les classer. 
- **Les variables ordinales** : elles prennent plusieurs modalit√©s et peuvent √™tre hi√©rarchis√©es entre elles. 
- **Les variables nominales** : elles prennent plusieurs modalit√©s et il est impossible de les classer. Elles sont tr√®s courantes dans le domaine du machine  learning 


## 2.2. Explication des coefficient, constante et r√©sidus 

Comme √©nonc√© dans la partie pr√©cedente, consid√©rons la cas classique d'une fonction affine :

$$y=ax+b$$

Ici, `a` et `b` sont des nombres r√©els. Ces deux nombres d√©finissent enti√®rement la courbe et permet donc d'obtenir une relation affine entre `x` et `y`. 

On rappel que `a` le coefficient directeur de la droite `y` et `b` repr√©sente l'ordonn√©e √† l'origine (nomm√©e intercept). 

En statistique, cette relation est √† la base des mod√®les dit lin√©aires, o√π une variable cible se d√©finit comme une somme de variables explicatives o√π chacune de ces derni√®res sont multipli√©s par un coefficient. Dans ce chapitre nous verrons uniquement le cas de la r√©gression avec une variable explicative. 


Dans ce mod√®le simple (√† une seule variable explicative), on suppose que la variable r√©ponse suit le mod√®le suivant :

$$y_i=\beta_0 + \beta_1 x_i + \varepsilon_i$$

On remarque la ressemblance avec la fonction affine pr√©sent√©e ci-dessus. La diff√©rence r√©side dans la notation (nous avons utilis√© des lettre grec) ainsi que dans l'existence d'un terme al√©atoire appel√© bruit $\varepsilon_i$ . 

Afin de consid√©rer le mod√®le, il est n√©cessaire de se placer sous certaines hypoth√®ses dont nous parlerons par la suite. 

Si nous prenons l'exemple d'un √©chantillons de logements √† Paris qui d√©crivent leur localisation, diverses caract√©ristiques ainsi que le montant du loyer. On pourrait imaginer une liste de prix `Y` ainsi qu'une liste de caract√©ristiques (observations) `X`. 

Les diff√©rents √©l√©ments qui interviennent dans la formule ci-dessus seraient donc :

- $ \beta_0$ : l'ordonn√©e √† l'origine (c‚Äôest √† dire le niveau 0 de `y` lorsque `x` vaut 0)
- $ \beta_1$ : le coefficient directeur, c'est le param√®tre du mod√®le qui mesure l‚Äôinfluence de `x` sur `y`, si `x` augmente de 1, `y` augmentera de $ \beta_1$
- $ x_i$ : l'observation $i$
- $ y_i$ : le $i$-√®me prix
- $ \varepsilon_i$ : le bruit al√©atoire li√©e ou r√©sidu √† la $i$-√®me observation

En effet, l‚Äô√©quation ci-dessus est la repr√©sentation d‚Äôun mod√®le statistique, il n‚Äôa pas la pr√©tention d‚Äô√™tre exact, il est vrai en moyenne, voil√† qui explique la pr√©sence du r√©sidu.

C'est donc avec ces param√®tres et en fonction des individus (ou nuage de point), que le mod√®le va trouver la droite qui se rapproche le plus possible tous les individus √† la fois. 


Quelques mots sur les r√©sidus, souvent not√© $\varepsilon$, ils correspondent √† l‚Äôerreur commise lors de la mod√©lisation. Cette erreur correspond √† toute l‚Äôinformation qui n‚Äôest pas expliqu√©e par le mod√®le, on suppose souvent que l‚Äôerreur suit une loi de probabilit√© particuli√®re. Nous reparlerons de ce concept d'erreur dans la partie suivante ü§ì

Dans ce cours nous ne verrons pas en d√©tail comment calculer les coefficients du mod√®le $\beta_1$ et $\beta_0$ mais sachez qu'ils peuvent se calculer avec les expressions suivantes :  

$\beta_1 = \frac{Cov(X,Y)}{V(X)}$

et

$\beta_0 = \bar{Y} - a.\bar{X}$

ou : 

- $Cov(X,Y)$ d√©signe la covariance entre les variables $X$ et $Y$ 
- $\bar{Y}$ et $\bar{X}$ d√©signent la moyenne des variables $Y$ et $X$ 


## 2.3. Les hypoth√®ses derri√®re une r√©gression lin√©aire : lin√©arit√©, homoscedasticit√© et normalit√© des variables

Quand vous construisez un mod√®le de machine learning, vous devrez √™tre conscient des hypoth√®ses que vous devez respecter pour que votre mod√®le fonctionne bien. Dans le cas inverse, vous aurez des performances d√©plorables. 

Voici donc les hypoth√®ses d‚Äôun mod√®le de r√©gression lin√©aire simple :


### 2.3.1. Lin√©arit√©

La premi√®re hypoth√®se est simple. Il faut que vos points suivent √† peu pr√®s une droite. En d‚Äôautres termes, vous devez vous assurer que votre variable d√©pendante suive une croissance lin√©aire √† mesure que vos variables ind√©pendantes augmentent.



### 2.3.2. Homoscedasticit√©

Au del√† de la complexit√© du mod√®le en lui m√™me, cela veut dire que la variance de vos points doit √™tre relativement la m√™me. Si vous avez une variance √©norme, cela veut dire que vous avez des points tr√®s √©loign√©s les uns des autres et que donc il sera difficile d‚Äôavoir une ligne qui soit repr√©sentative de votre dataset.



### 2.3.3. Normalit√© des variables

Les points doivent avoir une distribution normale (ou du moins √† peu de choses pr√®s). Vous n‚Äôaurez cependant rarement une distribution normale de vos points. Le tout est d‚Äôavoir une moyenne, une m√©diane et un mode qui ne soit pas trop √©loign√©s.  



## 2.4 Estimation par la m√©thode des moindres carr√©s 


Dans le langage statistique, un estimateur est une fonction permettant d'√©valuer un param√®tre inconnu (relatif √† une loi de probabilit√©) √† partir d'autre(s) param√®tre(s). **Dans notre cas notre estimateur va etre notre modele / notre fonctions scikit-learn** donc notre droite de r√©gression. 

Vous vous demandez s√ªrement comment on sait que la droite de notre mod√®le est celle qui se rapproche ‚ÄúLe plus‚Äù de chacun des points de notre dataset. Et bien, c‚Äôest gr√¢ce √† _la m√©thode des moindres carr√©s_. 
Cette m√©thode va donc nous servir √† calculer √† mesurer les erreurs entre notre droite de pr√©diction et nos donn√©es.  
Nous n‚Äôallons pas aller trop loin dans la d√©monstration de la formule. Ce qu‚Äôil y a √† comprendre est que l‚Äôalgorithme va chercher la distance minimum possible entre chaque point dans votre graphique via cette formule :


$$min\sum_{i=0}^{n} (y_i - \hat{y_i})^2 = min\sum_{i=0}^{n} (y_i - \beta_0 - \beta_1x_i)^2$$

Dans cette √©quation, $y_i$ repr√©sente chaque individu (ou point) de votre jeu de donn√©es alors que $\hat{y_i}$ repr√©sente la pr√©diction de votre mod√®le. En reprenant les m√™mes notations, $ \beta_0$ est l'ordonn√©e √† l'origine, $ \beta_1$ le coefficient directeur et $ x_i$ l'observation de la ligne $i$. 

Le fait de soustraire pour tout les individus $\hat{y_i}$ √† $y_i$ en mettant cette diff√©rence au carr√© revient √† calculer la distance entre la pr√©diction de notre mod√®le en un point et le point lui m√™me. Cette distance peut √™tre consid√©r√© comme une mesure de la distance entre les donn√©es exp√©rimentales et le mod√®le th√©orique qui pr√©dit ces donn√©es (l'erreur de pr√©diction de notre estimateur). Si on cherche la distance minimum pour tous les points on obtient donc la "meilleure" droite, ou du moins celle qui minimise l'erreur. 

Apr√®s plusieurs it√©rations, l'algorithme est capable de "trouver la meilleure formule de la meilleure droite possible" qui d√©crit votre jeu de donn√©es.


## 2.5. Analyse et validation du mod√®le : $R^2$ et $MSE$   

### 2.5.1. Le coefficient $R^2$ 

Maintenant que nous avons compris comment fonctionne le modele lin√©aire voyons **comment analyser notre r√©sultat et appr√©cier la qualit√© de notre pr√©diction**. 

Dans l'id√©e, nous aimerions obtenir une valeur num√©rique (une statistique "dans le jargon") qui nous indique **√† quel point la r√©gression lin√©aire a un sens sur nos donn√©es**. Pour cela, nous allons introduire quelques notations math√©matiques que nous d√©taillerons bien par la suite donc pas de panique. 

- La somme des carr√©s r√©siduels : $SCR=\sum_{i=0}^{n}(y_i-\hat{y_i})^2=\varepsilon^2 $ est un indicateur qui quantifie l‚Äôerreur commise par le mod√®le, ou en d‚Äôautres termes la portion de la dispersion de la variable cible qui n‚Äôest pas expliqu√©e par le mod√®le, d‚Äôo√π l‚Äôid√©e de r√©sidu.
- La somme des carr√©s totaux : $SCT=\sum_{i=0}^{n}(y_i-\bar{y})^2$ est un indicateur de la dispersion des valeurs de la variable cible $Y$ (dont les valeurs sont not√©es $(y_1, ... y_n)$ sur la population consid√©r√©e. 
- La somme des carr√©s expliqu√©s : $SCE=\sum_{i=0}^{n}(\hat{y_i} - \bar{y})^2 $ est un indicateur qui repr√©sente la quantit√© de dispersion de la variable cible qui est expliqu√©e par le mod√®le.  

Il est essentiel de bien comprendre ces valeurs car elles vont nous permettre de construire des m√©triques d‚Äô√©valuations des mod√®les de r√©gression lin√©aire simple et multiple. 

L'id√©e est de d√©composer la somme des carr√©s totaux comme la somme des carr√©s que le mod√®le explique, en plus de la somme des carr√©s qui sont li√©s aux r√©sidus (et donc que le mod√®le ne peut pas expliquer). On voit donc ici l'int√©r√™t de calculer un coefficient √† partir du $SCE$. Puisque l'on a la relation suivante :

$$SCT=SCE+SCR \text{ alors } 1=\frac{SCE}{SCT}+\frac{SCR}{SCT}$$


Plus les r√©sidus sont petits (et donc la r√©gression est "bonne"), plus $SCR$ devient petit et donc $SCE$ devient grand. Le sch√©ma inverse s'op√®re de la m√™me fa√ßon. Dans le meilleur des cas, on obtient $SCR=0$ et donc $SCE=SCT$ d'o√π le premier membre vaut $1$. Dans le cas contraite, $SCE=0$ et automatiquement, le premier membre est nul. C'est ainsi que l'on d√©finit le coefficient de d√©termination $R^2$ comme 
$$R^2=\frac{SCE}{SCT}=1-\frac{SCR}{SCT}$$

Pour r√©sumer, $SCT$ est la variance totale de la variable cible, qui peut se d√©composer en deux composantes : $SCE$ la variance expliqu√©e par le mod√®le, qui correspond √† la quantit√© de variance de nos estimation par rapport √† la moyenne r√©elle de la population observ√©e et $SCR$ qui est la somme des carr√©s des √©carts entre nos estimations est les valeurs r√©elles de la variables cible. En d‚Äôautres termes $SCT$ est la quantit√© d‚Äôinformation totale, $SCE$ est l‚Äôinformation expliqu√©e par le mod√®le et $SCR$ l‚Äôinformation qui reste √† expliquer, ou l‚Äôerreur commise.

Ainsi, $R^2 \in [0,1]$, et plus $R^2$ est proche de $1$, plus la r√©gression lin√©aire a du sens. Au contraire, si $R^2$ est proche de $0$, le mod√®le lin√©aire poss√®de un faible pouvoir explicatif.

On peut le calculer avec la fonction `r2_score` de `sklearn.metrics`. 


### 2.5.2. Le crit√®re $MSE$ 

Selon Wikipedia En statistiques, l‚Äôerreur quadratique moyenne d‚Äôun estimateur $\hat{\theta}$ d‚Äôun param√®tre $\theta$ ou mean squared error $MSE$
en anglais) est une mesure caract√©risant la ¬´ pr√©cision ¬ª de cet estimateur. Elle est plus souvent appel√©e ¬´ erreur quadratique ¬ª (¬´ moyenne ¬ª √©tant sous-entendu) ; elle est parfois appel√©e aussi ¬´ risque quadratique ¬ª.

Dans notre cas, l'estimateur sera notre algorithme `LinearRegression()` de `sklearn.linear_model`, l'objectif sera donc de mesurer sa pr√©cision et donc sa qualit√© de pr√©diction. On va pour cela faire la moyenne des erreurs au carr√©e avec la formule suivante : 

$$MSE = \frac{1}{n} \sum_{i=0}^{n} (y_i - \hat{y_i})^2 $$

Attention toutefois √† l'interpr√©tation de ce crit√®re, suivant votre jeu de donn√©es il sera plus convenable de prendre la racine carr√©e appel√© $RMSE$ pour root mean squared error. 

$$RMSE = [\frac{1}{n} \sum_{i=0}^{n} (y_i - \hat{y_i})^2 ]^\frac{1}{2} $$

On rappel que $x^\frac{1}{2}=\sqrt{x}$

Vous savez maintenant comment fonctionne une r√©gression lin√©aire simple et analyser ses r√©sultats. Passons maintenant √† la pratique avec la librairie scikit-learn. 


__________________________________________________ 

# Partie 3. La r√©gression lin√©aire simple en pratique avec la librairie scikit-learn 

Pour cette partie nous aurons besoin de plus de donn√©es que dans la partie pr√©c√©dente. On va donc pour cela g√©n√©rer des donn√©es lin√©aires avec la librairie numpy et les fonctions :  

- `numpy.arange()`: https://numpy.org/doc/stable/reference/generated/numpy.arange.html 
- `numpy.random.uniform()` : https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html 


```python
import numpy as np 
x=np.arange(75)
delta = np.random.uniform(-10,10, size=(75,))
y = 0.4 * x +3 + delta
```

On peut visualiser rapidement nos donn√©es avec la fonction `plot()` de  la librairie matplotlib

```python
import matplotlib.pyplot as plt 
plt.plot(x,y,"*")
plt.xlabel("Variable explicative 'x' ")
plt.ylabel("Variable cible 'y'")
plt.title("Nuage de point")
plt.savefig("./intuitive_scatter_bis.png")
```

![Nuage de points exemple](./intuitive_scatter_bis.png) 


On importe ensuite la librairie scikit-learn afin d'aller chercher l'algorithme de r√©gression sous forme d'une fonction. 

Scikit-learn est un package python qui contient la plupart des mod√®les statistiques que nous allons utiliser dans les cours. On l'importe gr√¢ce √† la commande suivante :


```python
from sklearn.linear_model import LinearRegression
```

Vous remarquerez que nous n'importons que le mod√®le qui nous int√©resse (le mod√®le lin√©aire) car c'est un package assez lourd ü§ì

On va ensuite cr√©er notre fonction de regression de la fa√ßon suivante : 

```python
linear_model = LinearRegression()
```

Pour optimiser les param√®tres du mod√®le (utiliser ce mod√®le g√©n√©rique et le faire coller √† nos donn√©es) en utilisant la m√©thode des moindres carr√©s :


```python
linear_model.fit(x.reshape(-1, 1),y)
```

Le parametre `x.reshape(-1, 1)` de la fonction `fit()` est pr√©sent car la fonction `LinearRegression()` de scikit-learn est faite pour la r√©gression multiple que nous verrons dans le prochain chapitre.  

On peut donc maintenant faire des pr√©dictions √† partir de donn√©es avec la fonction `predict()` tel que :


```
predictions = linear_model.predict(x)
```

Pour visualiser notre le mod√®le (droite de pr√©diction) et les donn√©es on peut utiliser la fonction suivante : 

```
def reg_plot(x,y,m):
    plt.scatter(x,y,c='blue',label="les donn√©es")
    plt.plot(x, m.predict(x.reshape(-1, 1)), color='red',label="droite de pr√©diction")
    plt.xlabel("Variable explicative 'x' ")
    plt.ylabel("Variable cible 'y'")
    plt.legend()
    return None 
```

et ensuite l'appliquer sur nos donn√©es tel que : 

```
reg_plot(x,y,linear_model)
```
![Nuage de points](./prediction.png) 

On peut aussi afficher les coefficients $R^2$ et $MSE$ tel que : 

```
y_pred = regr.predict(x)
print("Mean squared error: %.2f"
      % mean_squared_error(y, y_pred))
print("Root mean squared error: %.2f"
      % sqrt(mean_squared_error(y, y_pred)))
print("R square: %.2f"
      % r2_score(y, y_pred))
```